{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Traditional\" Text Classification with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this HW, we're going to experiment with a few \"traditional\" approaches to text classification. These approaches pre-date the deep learning revolution in Natural Language Processing, but are often quick and effective ways of training a text classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data, we're going to work with the [20 Newsgroups data set](http://qwone.com/~jason/20Newsgroups/), a classic collection of text documents that is often used as a benchmark for text classification models. The set contains texts about various topics, ranging from computer hardward to religion. Some of the topics are closely related to each other (such as \"IBM PC hardware\" and \"Mac hardware\"), while others are very different (such as \"religion\" or \"hockey\"). The 20 Newsgroups comes shipped with the [Scikit-learn machine learning library](https://scikit-learn.org/stable/), our main tool for this exercise. It has been split into training set of 11,314 texts and a test set of 7,532 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 11314\n",
      "Test texts: 7532\n"
     ]
    }
   ],
   "source": [
    "train_data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "print(\"Training texts:\", len(train_data.data))\n",
    "print(\"Test texts:\", len(test_data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (20%)\n",
    "\n",
    "The first step in the development of any NLP model is text preprocessing. This means we're going to transform our texts from word sequences to feature vectors. These feature vectors contain their values for each of a large number of features. Two important techniques are used to get weighted features of words: CountVectorizer and TfidfTransformer. Could you explain them and how to use them in NLP? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels for classification\n",
    "y_train, y_test = train_data.target, test_data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer-\n",
    "CountVectorizer is a way to tokenize words and build a dictionary or vocabulary using the words in a document. Words can be encoded using the dictionary formed.  \n",
    "Steps to use CountVectorizer- \n",
    "- Import CountVectorizer from sklearn.feature_extraction.text\n",
    "- Create an instance of CountVectorizer class\n",
    "- Call the fit_transform function to learn vocabulary from document and encode words as vectors\n",
    "- transform() test document to encode each word as a vector\n",
    "\n",
    "### TfidfTransformer-\n",
    "\n",
    "TFidfTransformer is used to get tf-idf scores of each word in a document. Pre-requisite to using tfidfTransformer is to use CountVectorizer(). Steps to use TfidfTransformer are as follows-\n",
    "- fit_transform() the CountVectorizer output vectors\n",
    "- Transform the test data using same fitted transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text data to feature vectors using CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_countVectorizer = count_vectorizer.fit_transform(train_data.data)\n",
    "X_test_countVectorizer = count_vectorizer.transform(test_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TfidfTransformer on vectors\n",
    "tfidf_transformer=TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_countVectorizer)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_countVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (40%)\n",
    "\n",
    "Next, we train a text classifier on the preprocessed training data. We're going to experiment with three classic text classification models: Naive Bayes, Support Vector Machines and Logistic Regression. Could you explain them and implment these classifiers with your preprocessed words from previous problem to determine classifier accuracy? Of course, we can play with other classifiers also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes-\n",
    "Naive Bayes classifier is based on Bayes' theorem. It is a probabilistic classifier model that assumes independence between each and all feature vectors i.e., training variables.\n",
    "Bayes' theorem states that for two events A and B,   \n",
    "            P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "### Support Vector Machines-\n",
    "Support Vector Machines define a hyperplane- a classifying boundary between different classes and SVM attempts to maximize the distance between boundary and classified data points. The hyperplane can be nonlinear too. SVM uses non-linear kernel for non-linear boundary between classes.\n",
    "\n",
    "### Logistic Regression- \n",
    "Logistic regression is specifically used when the output variable is categorical and when we expect the output to be positive only. It uses sigmoid function as its hypothesis.   \n",
    "            Z = Wx + b  \n",
    "            h(Z) = sigmoid(Z)  \n",
    "                  = 1/(1+exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_names = train_data.target_names\n",
    "\n",
    "# train_model function takes classifier as input, trains model on that classifier, and predicts output\n",
    "def train_model(clf, X_train, y_train, X_test, y_test):\n",
    "    print('#'*80)\n",
    "    print(\"Model to be trained: \", clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred, target_names=target_names))\n",
    "\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    print(clf_descr)\n",
    "    return clf_descr, pred, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training two transformed feature vectors on Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Model to be trained:  MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.80      0.52      0.63       319\n",
      "           comp.graphics       0.81      0.65      0.72       389\n",
      " comp.os.ms-windows.misc       0.82      0.65      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.67      0.78      0.72       392\n",
      "   comp.sys.mac.hardware       0.86      0.77      0.81       385\n",
      "          comp.windows.x       0.89      0.75      0.82       395\n",
      "            misc.forsale       0.93      0.69      0.80       390\n",
      "               rec.autos       0.85      0.92      0.88       396\n",
      "         rec.motorcycles       0.94      0.93      0.93       398\n",
      "      rec.sport.baseball       0.92      0.90      0.91       397\n",
      "        rec.sport.hockey       0.89      0.97      0.93       399\n",
      "               sci.crypt       0.59      0.97      0.74       396\n",
      "         sci.electronics       0.84      0.60      0.70       393\n",
      "                 sci.med       0.92      0.74      0.82       396\n",
      "               sci.space       0.84      0.89      0.87       394\n",
      "  soc.religion.christian       0.44      0.98      0.61       398\n",
      "      talk.politics.guns       0.64      0.94      0.76       364\n",
      "   talk.politics.mideast       0.93      0.91      0.92       376\n",
      "      talk.politics.misc       0.96      0.42      0.58       310\n",
      "      talk.religion.misc       0.97      0.14      0.24       251\n",
      "\n",
      "                accuracy                           0.77      7532\n",
      "               macro avg       0.83      0.76      0.76      7532\n",
      "            weighted avg       0.82      0.77      0.77      7532\n",
      "\n",
      "MultinomialNB\n"
     ]
    }
   ],
   "source": [
    "# Training on Naive Bayes Classifiers\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf_descr_NB_tfidf, pred_NB, score_NB =train_model(clf, X_train_tfidf, y_train, X_test_tfidf, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Model to be trained:  LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=0, tol=1e-05,\n",
      "          verbose=0)\n",
      "classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.80      0.81       319\n",
      "           comp.graphics       0.76      0.80      0.78       389\n",
      " comp.os.ms-windows.misc       0.77      0.73      0.75       394\n",
      "comp.sys.ibm.pc.hardware       0.71      0.76      0.74       392\n",
      "   comp.sys.mac.hardware       0.84      0.86      0.85       385\n",
      "          comp.windows.x       0.87      0.76      0.81       395\n",
      "            misc.forsale       0.83      0.91      0.87       390\n",
      "               rec.autos       0.92      0.91      0.91       396\n",
      "         rec.motorcycles       0.95      0.95      0.95       398\n",
      "      rec.sport.baseball       0.92      0.95      0.93       397\n",
      "        rec.sport.hockey       0.96      0.98      0.97       399\n",
      "               sci.crypt       0.93      0.94      0.93       396\n",
      "         sci.electronics       0.81      0.79      0.80       393\n",
      "                 sci.med       0.90      0.87      0.88       396\n",
      "               sci.space       0.90      0.93      0.92       394\n",
      "  soc.religion.christian       0.84      0.93      0.88       398\n",
      "      talk.politics.guns       0.75      0.92      0.82       364\n",
      "   talk.politics.mideast       0.97      0.89      0.93       376\n",
      "      talk.politics.misc       0.82      0.62      0.71       310\n",
      "      talk.religion.misc       0.75      0.61      0.68       251\n",
      "\n",
      "                accuracy                           0.85      7532\n",
      "               macro avg       0.85      0.85      0.85      7532\n",
      "            weighted avg       0.85      0.85      0.85      7532\n",
      "\n",
      "LinearSVC\n"
     ]
    }
   ],
   "source": [
    "# Training on SVM Classifiers\n",
    "\n",
    "clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf_descr_SVM, pred_SVM, score_SVM = train_model(clf, X_train_tfidf, y_train, X_test_tfidf, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and getting output of TfIdf vectors, CountVectorizer vectors on Logistic Regression Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Model to be trained:  LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
      "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
      "                     max_iter=300, multi_class='auto', n_jobs=None,\n",
      "                     penalty='l2', random_state=0, refit=True, scoring=None,\n",
      "                     solver='lbfgs', tol=0.0001, verbose=0)\n",
      "classification report:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.83      0.78      0.81       319\n",
      "           comp.graphics       0.76      0.81      0.78       389\n",
      " comp.os.ms-windows.misc       0.76      0.71      0.73       394\n",
      "comp.sys.ibm.pc.hardware       0.69      0.74      0.72       392\n",
      "   comp.sys.mac.hardware       0.83      0.85      0.84       385\n",
      "          comp.windows.x       0.85      0.77      0.81       395\n",
      "            misc.forsale       0.80      0.90      0.85       390\n",
      "               rec.autos       0.91      0.90      0.91       396\n",
      "         rec.motorcycles       0.97      0.95      0.96       398\n",
      "      rec.sport.baseball       0.93      0.95      0.94       397\n",
      "        rec.sport.hockey       0.97      0.98      0.97       399\n",
      "               sci.crypt       0.94      0.92      0.93       396\n",
      "         sci.electronics       0.77      0.80      0.79       393\n",
      "                 sci.med       0.90      0.84      0.87       396\n",
      "               sci.space       0.90      0.92      0.91       394\n",
      "  soc.religion.christian       0.84      0.93      0.88       398\n",
      "      talk.politics.guns       0.75      0.90      0.81       364\n",
      "   talk.politics.mideast       0.97      0.89      0.93       376\n",
      "      talk.politics.misc       0.80      0.62      0.70       310\n",
      "      talk.religion.misc       0.73      0.63      0.68       251\n",
      "\n",
      "                accuracy                           0.85      7532\n",
      "               macro avg       0.85      0.84      0.84      7532\n",
      "            weighted avg       0.85      0.85      0.85      7532\n",
      "\n",
      "LogisticRegressionCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrunalikhandat/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Training on Logistic Regression Classifiers\n",
    "clf =  LogisticRegressionCV(cv=5, random_state=0, max_iter=300, multi_class = 'auto')\n",
    "clf_descr_LR_tfidf, pred_LR, score_LR = train_model(clf, X_train_tfidf, y_train, X_test_tfidf, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensive evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem (40%)\n",
    "\n",
    "So far we've only looked at the accuracy of our models: the proportion of test examples for which their prediction is correct. This is fine as a first evaluation, but it doesn't give us much insight in what mistakes the models make and why. We'll therefore perform a much more extensive evaluation, in three steps. What are precision, recall, F-score and confusion matrix? And please using them to measure your trained classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "Confusion matrix displays the predictions as a matrix between true predictions and false predictions for each of the class.  \n",
    "True positives- The number of predictions that are classified as positives and are actually positives  \n",
    "False positives- The number of predictions that are predicted positive but are actually negative  \n",
    "True negatives- The number of predictions that are classified as negative and are also negative actually  \n",
    "False negatives- The number of predictions that are predicted negatives but are actually positive  \n",
    "\n",
    "#### Precision\n",
    "The number of true positives divided by the addition of the number of true positives and the number of false positives.   \n",
    "Precision = TP/(TP + FP)  \n",
    "\n",
    "#### Recall\n",
    "The number of true positives divided by the addition of the number of true positives and the number of false negatives.  \n",
    "Recall = TP/(TP + FN). \n",
    "#### F-score\n",
    "The F1 score is the harmonic mean of precision and recall.  \n",
    "F1-score = 2* Precision * Recall/(Precision + Recall)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extensive_evaluation function takes y_test and predicted y as input and prints classification report,\n",
    "# confusion matrix. The function returns precision score, recall score, F-1 score and suport\n",
    "def extensive_evaluation(y_test, pred, classifier_name):\n",
    "    print('#'*80)\n",
    "    print(\"Extensive Evaluation for \", classifier_name)\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"confusion matrix:\")\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "    print(confusion_matrix)\n",
    "    precision_score, recall_score, f1_score, support = precision_recall_fscore_support(y_test, pred, average='macro')\n",
    "    \n",
    "    return precision_score, recall_score, f1_score, support, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation paramters for Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Extensive Evaluation for  Naive Bayes\n",
      "confusion matrix:\n",
      "[[166   0   0   1   0   1   0   0   1   1   1   3   0   6   3 123   4   8\n",
      "    0   1]\n",
      " [  1 252  15  12   9  18   1   2   1   5   2  41   4   0   6  15   4   1\n",
      "    0   0]\n",
      " [  0  14 258  45   3   9   0   2   1   3   2  25   1   0   6  23   2   0\n",
      "    0   0]\n",
      " [  0   5  11 305  17   1   3   6   1   0   2  19  13   0   5   3   1   0\n",
      "    0   0]\n",
      " [  0   3   8  23 298   0   3   8   1   3   1  16   8   0   2   8   3   0\n",
      "    0   0]\n",
      " [  1  21  17  13   2 298   1   0   1   1   0  23   0   1   4  10   2   0\n",
      "    0   0]\n",
      " [  0   1   3  31  12   1 271  19   4   4   6   5  12   6   3   9   3   0\n",
      "    0   0]\n",
      " [  0   1   0   3   0   0   4 364   3   2   2   4   1   1   3   3   4   0\n",
      "    1   0]\n",
      " [  0   0   0   1   0   0   2  10 371   0   0   4   0   0   0   8   2   0\n",
      "    0   0]\n",
      " [  0   0   0   0   1   0   0   4   0 357  22   0   0   0   2   9   1   1\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   4 387   1   0   0   1   5   0   0\n",
      "    0   0]\n",
      " [  0   2   1   0   0   1   1   3   0   0   0 383   1   0   0   3   1   0\n",
      "    0   0]\n",
      " [  0   4   2  17   5   0   2   8   7   1   2  78 235   3  11  15   2   1\n",
      "    0   0]\n",
      " [  2   3   0   1   1   3   1   0   2   3   4  11   5 292   6  52   6   4\n",
      "    0   0]\n",
      " [  0   2   0   1   0   3   0   2   1   0   1   6   1   2 351  19   4   0\n",
      "    1   0]\n",
      " [  2   0   0   0   0   0   0   0   1   0   0   0   0   1   2 392   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   2   0   1   1   0  10   0   0   1   6 341   1\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   1   0   2   0   0   0  24   3 344\n",
      "    1   0]\n",
      " [  2   0   0   0   0   0   0   1   0   0   1  11   0   1   7  35 118   5\n",
      "  129   0]\n",
      " [ 33   2   0   0   0   0   0   0   0   1   1   3   0   4   4 131  29   5\n",
      "    3  35]]\n"
     ]
    }
   ],
   "source": [
    "precision_score_NB, recall_score_NB, f1_score_NB, support_NB, score_NB = extensive_evaluation(y_test, pred_NB, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR NAIVE BAYES CLASSIFIER: \n",
      "--------------------------------------------------------------------------------\n",
      "Precision score:  0.8255310124210137\n",
      "Recall score:  0.756525006352595\n",
      "F-1 score:  0.7557542971333199\n",
      "Accuracy:  0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR NAIVE BAYES CLASSIFIER: \")\n",
    "print('-'*80)\n",
    "print(\"Precision score: \",precision_score_NB)\n",
    "print(\"Recall score: \",recall_score_NB )\n",
    "print(\"F-1 score: \", f1_score_NB)\n",
    "print(\"Accuracy: \", score_NB)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation parameters for Linear SVM Classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Extensive Evaluation for  SVM Classifier\n",
      "confusion matrix:\n",
      "[[254   1   0   1   0   2   1   0   2   0   0   1   1   6   7  22   0   1\n",
      "    1  19]\n",
      " [  1 313  12   8   5  17   3   2   1   3   1   4   9   0   4   2   0   1\n",
      "    0   3]\n",
      " [  0  17 288  40   7  13   4   1   0   4   0   2   4   2   5   2   0   0\n",
      "    1   4]\n",
      " [  0  14  20 297  21   1  13   2   1   1   0   1  19   0   1   0   0   0\n",
      "    0   1]\n",
      " [  0   5   4  19 330   0   9   0   0   2   1   1   9   1   0   0   2   0\n",
      "    1   1]\n",
      " [  1  35  38   3   3 302   3   1   2   0   0   0   1   1   4   0   1   0\n",
      "    0   0]\n",
      " [  0   1   1  10   7   0 353   5   1   2   1   1   5   1   0   0   0   0\n",
      "    1   1]\n",
      " [  0   1   0   5   1   1  10 359   6   2   0   0   6   1   0   0   2   0\n",
      "    2   0]\n",
      " [  0   0   0   1   0   0   4   9 380   1   0   0   1   1   0   1   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   4   2   0 378  10   0   1   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   1   1   0   1   0   0   3 392   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   3   1   0   2   2   4   2   1   3   0 372   2   0   0   0   2   0\n",
      "    1   0]\n",
      " [  0   6   5  23   9   1   5   5   3   2   1   8 310   8   2   1   2   1\n",
      "    0   1]\n",
      " [  3   5   3   4   2   2   3   0   1   4   1   1   9 343   1   3   1   3\n",
      "    5   2]\n",
      " [  0   7   0   1   1   2   1   0   0   0   1   1   4   5 368   1   1   0\n",
      "    1   0]\n",
      " [  4   0   2   1   0   0   0   0   0   1   0   0   3   2   3 372   0   0\n",
      "    0  10]\n",
      " [  1   0   0   1   1   0   2   0   1   2   0   4   0   3   2   0 334   1\n",
      "    8   4]\n",
      " [ 11   1   0   0   1   3   0   1   0   3   0   0   0   0   1   6   2 335\n",
      "   12   0]\n",
      " [  3   1   0   0   1   1   1   1   0   0   0   3   0   3   5   4  88   2\n",
      "  192   5]\n",
      " [ 30   2   1   1   0   0   3   0   0   1   0   0   0   3   4  31  12   1\n",
      "    8 154]]\n"
     ]
    }
   ],
   "source": [
    "precision_score_SVM, recall_score_SVM, f1_score_SVM, support_SVM, score_SVM = extensive_evaluation(y_test, pred_SVM, \"SVM Classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR SVM CLASSIFIER USING LINEAR KERNEL: \n",
      "--------------------------------------------------------------------------------\n",
      "Precision score:  0.8513737648934537\n",
      "Recall score:  0.845836571817482\n",
      "F-1 score:  0.8464878396886357\n",
      "Accuracy:  0.8531598513011153\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR SVM CLASSIFIER USING LINEAR KERNEL: \")\n",
    "print('-'*80)\n",
    "print(\"Precision score: \",precision_score_SVM )\n",
    "print(\"Recall score: \", recall_score_SVM)\n",
    "print(\"F-1 score: \", f1_score_SVM)\n",
    "print(\"Accuracy: \", score_SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Parameters for Logistic Regression classifier models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Extensive Evaluation for  Logistic Regression\n",
      "confusion matrix:\n",
      "[[250   1   0   2   0   2   1   0   0   0   0   1   0   5   6  22   0   3\n",
      "    1  25]\n",
      " [  1 315   9   9   6  16   6   3   0   3   0   4   7   0   4   2   0   1\n",
      "    1   2]\n",
      " [  0  20 278  41  12  15   3   1   0   3   0   2   4   2   4   3   0   0\n",
      "    2   4]\n",
      " [  0  12  18 292  19   2  17   4   1   1   0   1  24   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   5   6  20 326   2   9   0   0   2   0   2   9   1   1   0   2   0\n",
      "    0   0]\n",
      " [  0  31  36   5   3 305   5   1   1   0   0   0   2   1   4   0   1   0\n",
      "    0   0]\n",
      " [  0   1   3  13   6   0 350   5   1   1   1   0   7   1   0   0   0   0\n",
      "    0   1]\n",
      " [  0   1   0   3   1   1  10 357   6   1   0   0   9   1   1   0   3   0\n",
      "    2   0]\n",
      " [  0   1   0   1   0   0   4  10 379   1   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   1   0   0   0   4   2   0 377  10   0   1   0   0   0   0   0\n",
      "    1   1]\n",
      " [  0   0   0   0   1   1   2   0   0   3 390   0   0   0   0   0   1   0\n",
      "    1   0]\n",
      " [  1   3   1   2   2   3   4   2   1   4   0 366   5   0   0   0   2   0\n",
      "    0   0]\n",
      " [  0   5   6  25   9   0   9   3   1   0   1   5 315   8   3   1   1   0\n",
      "    0   1]\n",
      " [  3   9   3   4   2   3   4   1   1   4   1   1  14 333   1   3   1   2\n",
      "    5   1]\n",
      " [  0   9   0   1   2   2   2   0   0   0   0   0   4   6 364   1   1   0\n",
      "    1   1]\n",
      " [  4   0   2   1   0   0   0   0   0   1   0   0   3   2   3 371   0   0\n",
      "    0  11]\n",
      " [  1   0   0   2   1   0   2   1   0   3   0   5   0   4   2   0 326   1\n",
      "   11   5]\n",
      " [  8   1   0   0   0   4   0   0   1   2   0   0   2   1   2   7   2 333\n",
      "   13   0]\n",
      " [  2   2   0   0   1   1   1   2   0   0   0   3   0   3   5   4  85   2\n",
      "  191   8]\n",
      " [ 30   0   1   1   0   0   4   0   0   1   0   0   0   3   3  27  11   1\n",
      "   10 159]]\n"
     ]
    }
   ],
   "source": [
    "precision_score_LR, recall_score_LR, f1_score_LR, support_LR, score_LR = extensive_evaluation(y_test, pred_LR, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR LOGISTIC REGRESSION CLASSIFIER: \n",
      "--------------------------------------------------------------------------------\n",
      "Precision score:  0.8450460355737446\n",
      "Recall score:  0.83973337274024\n",
      "F-1 score:  0.8405791662355078\n",
      "Accuracy:  0.8466542750929368\n"
     ]
    }
   ],
   "source": [
    "print(\"FOR LOGISTIC REGRESSION CLASSIFIER: \")\n",
    "print('-'*80)\n",
    "print(\"Precision score: \", precision_score_LR )\n",
    "print(\"Recall score: \",recall_score_LR )\n",
    "print(\"F-1 score: \", f1_score_LR)\n",
    "print(\"Accuracy: \", score_LR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#sphx-glr-auto-examples-text-plot-document-classification-20newsgroups-py\n",
    "- https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "- https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "- https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/\n",
    "- https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
